{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 - Train User-specific Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "<div style=\"text-align:justify; width: 97%\">\n",
    "In this task, we will complete the implementation of a full-stack P300 speller. We start by training the P300 signals classifier using stepwise linear discriminant analysis algorithm (SWLDA) in the offline portion. The classifier will classify whether the EEG signals recorded in a certain time interval come from the target stimulus, which corresponds to cases where the desired character is flashed. The assumption here is that only the target stimulus will elicit a unique waveform called P300 waves. The accuracy of the classifier plays a crucial role in the P300 speller's performance since it determines how precisely the speller can recognize the intended characters in the user's mind.\n",
    "</br>\n",
    "\n",
    "During the online portion (using the speller in a real-time scenario), we initialize the probability of each character either from a uniform distribution or a language model. To \"type\" each character via the P300 speller, users are asked to focus on the desired character and count the number of times it gets flashed. The screen will flash multiple characters each time, which will correspondingly trigger the brain to generate P300 waves or not. The classifier will process these signals and give the classification score. The score will be used to update the probabilities with Bayesian inference until a threshold is met or the maximum flashes are reached. The intuition here is that the P300 speller will increase the probabilities of those characters whose scores indicate that they are more likely to be in the class of targets, while at the same time, lowering the probabilities of those who are more likely to be non-targets. In the end, the character with the highest probability will be selected.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    direction TB\n",
    "    subgraph \"offline portion\"\n",
    "    direction LR\n",
    "    a1(\"Feed <br> training data\")-->a2(\"Train classifier\")\n",
    "    a2-->a3(\"Get distribution of <br> classifier scores <br> for targets and non-targets\")\n",
    "    a3-- KDE -->a4(\"Derive PDF of <br> H0 and H1\")\n",
    "    end\n",
    "    subgraph \"online portion\"\n",
    "    direction LR\n",
    "    b1(\"Initialize <br> characters <br> probabilities\")-->b2(\"New flash\")\n",
    "    b2(\"New flash\")-->b3(\"Record <br> EEG signals\")\n",
    "    b3(\"Record <br> EEG signals\")-->b4(\"Return <br> classifier scores\")\n",
    "    b4(\"Return <br> classifier scores\")-->b5(\"Update characters <br> probabilities\")\n",
    "    b5(\"Bayesian update <br> characters <br> probabilities\")-->b6{\"Decide whether <br> to stop\"}\n",
    "    b6-- Yes -->b7(\"Select character\")\n",
    "    b6-- No  -->b2\n",
    "    end\n",
    "    a4-->b5\n",
    "```\n",
    "\n",
    "</center>\n",
    "\n",
    "<caption><center><b>Figure 1</b>: The workflow of the P300 speller development with the DS stopping rule </font></center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Python standard libraries\n",
    "import math\n",
    "\n",
    "# Packages for computation and modelling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import mne\n",
    "import pickle\n",
    "\n",
    "# Packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Self-defined packages\n",
    "from swlda import SWLDA\n",
    "import utils\n",
    "\n",
    "# Magic command to reload packages whenever we run any later cells\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; width: 97%\">\n",
    "\n",
    "Now, let's start by defining some \"constant variables\" to set up the general configuration. First, we set `board` as the keyboard interface for users to \"type\" characters. The keyboard is displayed on the screen during the experiment, and it's usually a $6 \\times 6$ or $9 \\times 8$ matrix. Here, we use a $9 \\times 8$ keyboard, with 9 rows and 8 columns. The layout looks like this:\n",
    "\n",
    "```\n",
    "+---+---+---+----+\n",
    "| A | B | C | .. |\n",
    "+---+---+---+----+\n",
    "| I | J | K | .. |\n",
    "+---+---+---+----+\n",
    "| Q | R | S | .. |\n",
    "+---+---+---+----+\n",
    "| : | : | : |    |\n",
    "+---+---+---+----+\n",
    "```\n",
    "\n",
    "After setting up the board, we can define the number of rows(`n_rows`), the number of columns (`n_cols`), and the number of characters (`M`). These variables turn out to be very useful in the later analysis. Finally, we initilize the starting probability of each character equally as $1/M$. The probabilities are stored in a `np.array` with the same layout as the `board`. Note that there are some other ways to set the initial probs, but using the uniform distribution here is convenient and also intuitive.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD = [[\"A\",    \"B\",  \"C\",   \"D\",    \"E\",    \"F\",     \"G\",    \"H\"    ],\n",
    "         [\"I\",    \"J\",  \"K\",   \"L\",    \"M\",    \"N\",     \"O\",    \"P\"    ],\n",
    "         [\"Q\",    \"R\",  \"S\",   \"T\",    \"U\",    \"V\",     \"W\",    \"X\"    ],\n",
    "         [\"Y\",    \"Z\",  \"Sp\",  \"1\",    \"2\",    \"3\",     \"4\",    \"5\"    ],\n",
    "         [\"6\",    \"7\",  \"8\",   \"9\",    \"0\",    \"Prd\",   \"Ret\",  \"Bs\"   ],\n",
    "         [\"?\",    \",\",  \";\",   \"\\\\\",   \"/\",    \"+\",     \"-\",    \"Alt\"  ],\n",
    "         [\"Ctrl\", \"=\",  \"Del\", \"Home\", \"UpAw\", \"End\",   \"PgUp\", \"Shft\" ],\n",
    "         [\"Save\", \"'\",  \"F2\",  \"LfAw\", \"DnAw\", \"RtAw\",  \"PgDn\", \"Pause\"],\n",
    "         [\"Caps\", \"F5\", \"Tab\", \"EC\",   \"Esc\",  \"email\", \"!\",    \"Sleep\"]]\n",
    "BOARD  = np.array(BOARD)\n",
    "N_ROWS = BOARD.shape[0]  # number of rows\n",
    "N_COLS = BOARD.shape[1]  # number of columns\n",
    "M = N_ROWS * N_COLS      # the number of chars on the board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; width: 97%\">\n",
    "\n",
    "Let's define more configuration variables. `paradigm` refers to the displaying paradigm we are using. There are three commonly used paradigms - Row-Column (RC), Checkerboard (CB), and Random-paradigm (RD). I personally find this [article](https://sapienlabs.org/lab-talk/implementations-of-the-p300-bci-speller/) to be very helpful as a brief introduction to the three.\n",
    "</br>\n",
    "\n",
    "Next, we define `NUM_TIMESTAMPS` and `EPOCH_SIZE`. It's hard to explain these two without the context, so let's move back to the sampling procedure. As we know, the P300 speller tries to capture the P300 wave, a special event-related potential (ERP) elicited by the infrequent target stimuli among high-probability non-target items. As Figure 2 shows, The EEG of the P300 surfaces as a positive deflection in voltage with a latency (delay between stimulus and response) of roughly $250$ to $500$ ms (Polich, 2009). On the opposite, the EEG of normal waves doesn't have this shape. Given this, we set the length of the observation time window to be $800$ ms, starting from the release of the stimuli till $800$ ms later. In `MNE`, they introduce the data structure `Epoch` to manage the EEG data in equal-duration chunks. The official [tutorial](https://mne.tools/dev/auto_tutorials/epochs/10_epochs_overview.html) on `Epoch` should suffice to serve as a quick reference here.\n",
    "</br>\n",
    "\n",
    "Now that we have the nicely chopped data chunk (`Epoch`), let's work on sampling the electrical signals. The default sampling frequency is $256$ Hz. In other words, we take a snapshot of the EEG every $1/256$ seconds to record a voltage value. This means we will have roughly $206$ ($256 \\times 0.8$) data points (or timestamps) in one `Epoch`. If we use all the timestamps to build the classifier, the model will be very heavy and hard to train. Therefore, we choose to **average** the voltage value every $13$ timestamps, which will give us $15$ averaged observations in each `Epoch`. The last $11$ timestampes are not important, so we can just drop them away. At this point, we should be clear about the meaning of the two variables, `NUM_TIMESTAMPS` and `EPOCH_SIZE`. These two specify that we need the signals of the first $195$ ($13 \\times 15$) timestamps to compute $15$ observations in each `Epoch`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"./images/target_vs_nontarget_eeg.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "</center>\n",
    "\n",
    "<caption><center><b>Figure 2</b>: The average EEG of target response and non-target response (8 core electrodes) </font></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; width: 97%\">\n",
    "\n",
    "The original `.edf` file contains the EEG signals collected from a bunch of electrodes, but we only need the 8 core electrodes according to Krusienski et al (2006). `CORE_CHANNELS` specifies the name of these core electrodes. Note that these electrodes are called \"channels\" in `MNE`, and to get their exact names, we need to read/load the `.edf` data as a `Raw` object first, and then retrieve the attribute of `info` to get a brief summary of the raw data, including the channels information.\n",
    "\n",
    "Finally, to scale up the data analysis process, we organize the data in a systematic naming convention. Each experiment object has a training session and a testing session. In each session, each one uses the P300 speller to type some words. `NUM_TRAIN_WORDS` and `NUM_TEST_WORDS` define the number of words to type during training and testing respectively. We have 13 participants in total, and they are indexed by the variable `obj` here. Keep in mind that `obj` is not consecutive, so we need to explicitly store these indices in a list when we process all participants together later. But now, let's just focus on Participant #01, and see his/her performance.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm       = 'RC'  # display paradigm ('RC', 'CB', or 'RD')\n",
    "NUM_TIMESTAMPS = 195   # number of timestamps in each window to record signals\n",
    "EPOCH_SIZE     = 15    # required number of features in every epoch\n",
    "CORE_CHANNELS  = ('EEG_Fz', 'EEG_Cz',  'EEG_P3',  'EEG_Pz',\n",
    "                  'EEG_P4', 'EEG_PO7', 'EEG_PO8', 'EEG_Oz')\n",
    "NUM_CORE_CHANNELS  = len(CORE_CHANNELS)  # number of core eletrodes\n",
    "NUM_TRAIN_WORDS = 5 # number of training words for one participant\n",
    "NUM_TEST_WORDS  = 5 # number of testing words for one participant\n",
    "\n",
    "obj = 1 # the index of experiment object (participant)\n",
    "obj = str(obj) if obj > 10 else '0'+str(obj)\n",
    "directory = '/Users/zionshane/Desktop/Duke/Research/BCI_data/EDFData-StudyA'\n",
    "obj_directory = directory + f'/A{obj}/SE001'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; width: 97%\">\n",
    "\n",
    "Starting from reading the input `.edf` data files, there are several steps ahead before finishing the preparation of the training/testing datasets. To structurize the code here, We wrap everything up into a function called `load_data`. Here is a short summary of what it does.\n",
    "\n",
    "First, complete the path to the target file by setting the session (training/testing), the participant, and the paradigm given by the parameters. Then, loop through all the words in the directory (each word corresponds to a file). For each one, we:\n",
    "1. Read and load the input `.edf` file, and filter out the noise signals of a certain frequency.\n",
    "2. Select the core channels. Then, Build the epochs by dividing the data into many $800$ ms chunks, each of which starts from the beginning of each simulation. Tag these epochs with \"targets\" or \"non-targets\". Add all these epochs to a giant list.\n",
    "\n",
    "Now, after we collect all the epochs, we loop through the list, and for each one, we:\n",
    "1. Downsample (average) the data so each epoch now has 15 observations for each core channel. Then, concatenate these 8 core channels together to get $120 \\ (=8 \\times 15)$ features.\n",
    "2. Separate the data into features and responses. Collect all features together, and all responses together. Return the two as the `all_features` (`np.array` with size of (`num_records`, `num_features`)) and `all_response` (`np.array` with size of (`num_records`, 1)).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features,train_response = utils.load_data(dir=obj_directory,\n",
    "                                                obj=obj,\n",
    "                                                num_timestamps=NUM_TIMESTAMPS,\n",
    "                                                epoch_size=EPOCH_SIZE,\n",
    "                                                num_channels=NUM_CORE_CHANNELS,\n",
    "                                                type=paradigm,\n",
    "                                                mode='train',\n",
    "                                                num_words=NUM_TRAIN_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features,test_response   = utils.load_data(dir=obj_directory,\n",
    "                                                obj=obj,\n",
    "                                                num_timestamps=NUM_TIMESTAMPS,\n",
    "                                                epoch_size=EPOCH_SIZE,\n",
    "                                                num_channels=NUM_CORE_CHANNELS,\n",
    "                                                type=paradigm,\n",
    "                                                mode='test',\n",
    "                                                num_words=NUM_TEST_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the classifier for participant #01 is 0.851\n"
     ]
    }
   ],
   "source": [
    "clf = SWLDA(penter=0.1, premove=0.15)\n",
    "clf.fit(train_features, train_response)\n",
    "auc = clf.test(test_features, test_response)\n",
    "print(f'AUC of the classifier for participant #{obj} is {auc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the classifier as a standalone model file\n",
    "with open(f'./model/A{obj}-model.pkl','wb') as f:\n",
    "    pickle.dump(clf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(clf.test(train_features), columns=['score'])\n",
    "scores['is_target'] = train_response.astype('int')\n",
    "mu_1, std_1 = norm.fit(data=scores.loc[scores['is_target'] == 1]['score'])\n",
    "mu_0, std_0 = norm.fit(data=scores.loc[scores['is_target'] == 0]['score'])\n",
    "var_1 = std_1**2\n",
    "var_0 = std_0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = directory+'/A01/SE001/Test/RC/A01_SE001RC_Test06.edf'\n",
    "raw_data = mne.io.read_raw_edf(test_file, preload=True, verbose=False)\n",
    "\n",
    "stim_events = mne.find_events(raw=raw_data,\n",
    "                              stim_channel='StimulusBegin',\n",
    "                              verbose=False)\n",
    "eeg_channels = mne.pick_channels_regexp(raw_data.info['ch_names'], 'EEG')\n",
    "raw_data.notch_filter(freqs=60, picks=eeg_channels, verbose=False)\n",
    "test_epochs = utils.get_core_epochs(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In test #06, Participant #01 wants to type \"DRIVING\".\n"
     ]
    }
   ],
   "source": [
    "current_target_events = mne.find_events(raw_data, stim_channel='CurrentTarget',\n",
    "                                        verbose=False)\n",
    "current_target_appears = current_target_events[:,0]\n",
    "current_target = current_target_events[:,2]\n",
    "truth = utils.eventIDs_to_strings(BOARD, current_target)\n",
    "print(f'In test #06, Participant #{obj} wants to type \"{truth}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_events = mne.find_events(raw_data, stim_channel='PhaseInSequence',\n",
    "                                verbose=False)\n",
    "phases_appears = phases_events[:,0]\n",
    "phases = phases_events[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1032,  1160,  4968,  5864,  9672, 10568, 14376, 15272, 19080,\n",
       "       19976, 23784, 24680, 28488, 29384, 33192])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases_appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1160,  5864, 10568, 15272, 19976, 24680, 29384])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_target_appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1160, 4968),\n",
       " (5864, 9672),\n",
       " (10568, 14376),\n",
       " (15272, 19080),\n",
       " (19976, 23784),\n",
       " (24680, 28488),\n",
       " (29384, 33192)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "during_trail_phases = []\n",
    "for i in range(1, len(phases), 2):\n",
    "    start = phases_appears[i]\n",
    "    end = phases_appears[i+1]\n",
    "    during_trail_phases.append((start, end))\n",
    "\n",
    "during_trail_phases # check results [passed!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_response = utils.split_data(test_epochs,\n",
    "                                                n_channels=NUM_CORE_CHANNELS,\n",
    "                                                n_times=NUM_TIMESTAMPS,\n",
    "                                                n_samples=EPOCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(833, 120)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_begin_events = mne.find_events(raw=raw_data, stim_channel='StimulusBegin',\n",
    "                                    verbose=False)\n",
    "stim_begin_time   = stim_begin_events[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flashing_schedule = {time:[] for time in stim_begin_time}\n",
    "for i in range(N_ROWS):\n",
    "    for j in range(N_COLS):\n",
    "        ch = BOARD[i][j]\n",
    "        ch_index = N_COLS * i + j + 1\n",
    "        # Find stimulus events and target stimulus events.\n",
    "        # Non-zero value in `StimulusBegin` indicates stimulus onset.\n",
    "        stim_events       = mne.find_events(raw=raw_data,\n",
    "                                            stim_channel='StimulusBegin',\n",
    "                                            verbose=False)\n",
    "        # Non-zero value in `StimulusType` if is target stimulus event.\n",
    "        flashed_ch_events = mne.find_events(raw=raw_data,\n",
    "                                            stim_channel=f'{ch}_{i+1}_{j+1}',\n",
    "                                            verbose=False)\n",
    "\n",
    "        # Label flashed character events.\n",
    "        flashed_ch_time = np.isin(stim_events[:,0], flashed_ch_events[:,0])\n",
    "        stim_events[flashed_ch_time,2]  = ch_index\n",
    "        stim_events[~flashed_ch_time,2] = -1 # placeholder\n",
    "        for k in range(len(stim_begin_time)):\n",
    "            if stim_events[k, 2] != -1:\n",
    "                flashing_schedule[stim_events[k, 0]].append(ch_index)\n",
    "\n",
    "# flashing_schedule # check results [passed!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_scores = clf.test(data=test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEQ = 7\n",
    "T_MAX = (N_ROWS + N_COLS) * NUM_SEQ # maximum number of flashes in a trial\n",
    "ACTUAL_T_MAX = int(len(stim_begin_time)/len(truth))\n",
    "P_threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 56 flashes to stop.\n",
      "The estimated choice C* is \"D\" with probability of 0.912\n",
      "For reference, the probability of the true target \"D\" is 0.912\n"
     ]
    }
   ],
   "source": [
    "P_all = np.ones(shape=(N_ROWS, N_COLS)) * (1/M) # Initialize probs\n",
    "num_flashes = 0\n",
    "target_index = current_target[0]\n",
    "target_loc   = ((target_index - 1) // N_COLS, (target_index - 1) %  N_COLS)\n",
    "for k in range(len(stim_begin_time)):\n",
    "    num_flashes += 1\n",
    "    time = stim_begin_time[k]\n",
    "    flashed = flashing_schedule[time]\n",
    "    # Generate the classifier score\n",
    "    y = clf_scores[k]\n",
    "    # Update probabilities\n",
    "    for i in range(N_ROWS):\n",
    "        for j in range(N_COLS):\n",
    "            ch_index = N_COLS * i + j + 1\n",
    "            if (ch_index in flashed):\n",
    "                likelihood = stats.norm.pdf(x=y, loc=mu_1, scale=std_1)\n",
    "            else:\n",
    "                likelihood = stats.norm.pdf(x=y, loc=mu_0, scale=std_0)\n",
    "            P_all[i, j] = P_all[i, j] * likelihood\n",
    "    # Normalize P_all\n",
    "    P_all = P_all / P_all.sum()\n",
    "    # Check if can stop\n",
    "    if P_all.max() >= P_threshold:\n",
    "        break\n",
    "    if (time == current_target_appears[1]):\n",
    "        break\n",
    "\n",
    "print(f'It takes {num_flashes} flashes to stop.')\n",
    "max_loc = np.unravel_index(P_all.argmax(), P_all.shape)\n",
    "print('The estimated choice C* is \"%s\" with probability of %0.3f'\n",
    "      % (BOARD[max_loc], P_all[max_loc]))\n",
    "print('For reference, the probability of the true target \"%s\" is %0.3f'\n",
    "      % (BOARD[target_loc], P_all[target_loc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial #1:\n",
      "It takes 56 flashes to stop.\n",
      "The estimated choice C* is \"D\" with probability of 0.912\n",
      "For reference, the probability of the true target \"D\" is 0.912\n",
      "Trial #2:\n",
      "It takes 49 flashes to stop.\n",
      "The estimated choice C* is \"R\" with probability of 0.959\n",
      "For reference, the probability of the true target \"R\" is 0.959\n",
      "Trial #3:\n",
      "It takes 31 flashes to stop.\n",
      "The estimated choice C* is \"I\" with probability of 0.907\n",
      "For reference, the probability of the true target \"I\" is 0.907\n",
      "Trial #4:\n",
      "It takes 40 flashes to stop.\n",
      "The estimated choice C* is \"V\" with probability of 0.973\n",
      "For reference, the probability of the true target \"V\" is 0.973\n",
      "Trial #5:\n",
      "It takes 51 flashes to stop.\n",
      "The estimated choice C* is \"I\" with probability of 0.900\n",
      "For reference, the probability of the true target \"I\" is 0.900\n",
      "Trial #6:\n",
      "It takes 71 flashes to stop.\n",
      "The estimated choice C* is \"N\" with probability of 0.999\n",
      "For reference, the probability of the true target \"N\" is 0.999\n",
      "Trial #7:\n",
      "It takes 52 flashes to stop.\n",
      "The estimated choice C* is \"G\" with probability of 0.977\n",
      "For reference, the probability of the true target \"G\" is 0.977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'truth': ['D', 'R', 'I', 'V', 'I', 'N', 'G'],\n",
       " 'select': ['D', 'R', 'I', 'V', 'I', 'N', 'G'],\n",
       " 'times': [56, 49, 31, 40, 51, 71, 52]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trail_perform = {'truth':list(truth), 'select':[], 'times':[]}\n",
    "\n",
    "for trail in range(len(during_trail_phases)):\n",
    "    print(f'Trial #{trail+1}:')\n",
    "    P_all = np.ones(shape=(N_ROWS, N_COLS)) * (1/M) # Initialize probs\n",
    "    num_flashes = 0\n",
    "    target_index = current_target[trail]\n",
    "    target_loc   = ((target_index - 1) // N_COLS, (target_index - 1) %  N_COLS)\n",
    "    start, end = during_trail_phases[trail]\n",
    "    time = start\n",
    "    k = 0\n",
    "\n",
    "    while time <= end:\n",
    "        num_flashes += 1\n",
    "        flashed = flashing_schedule[time]\n",
    "        # Generate the classifier score\n",
    "        y = clf_scores[trail*ACTUAL_T_MAX + k]\n",
    "        # Update probabilities\n",
    "        for i in range(N_ROWS):\n",
    "            for j in range(N_COLS):\n",
    "                ch_index = N_COLS * i + j + 1\n",
    "                if (ch_index in flashed):\n",
    "                    likelihood = stats.norm.pdf(x=y, loc=mu_1, scale=std_1)\n",
    "                else:\n",
    "                    likelihood = stats.norm.pdf(x=y, loc=mu_0, scale=std_0)\n",
    "                P_all[i, j] = P_all[i, j] * likelihood\n",
    "        # Normalize P_all\n",
    "        P_all = P_all / P_all.sum()\n",
    "        # Check if can stop\n",
    "        if P_all.max() >= P_threshold:\n",
    "            break\n",
    "        else:\n",
    "            k += 1\n",
    "            if trail*ACTUAL_T_MAX + k == len(stim_begin_time):\n",
    "                break\n",
    "            else:\n",
    "                time = stim_begin_time[trail*ACTUAL_T_MAX + k]\n",
    "\n",
    "    max_loc = np.unravel_index(P_all.argmax(), P_all.shape)\n",
    "    trail_perform['select'].append(BOARD[max_loc])\n",
    "    trail_perform['times'].append(num_flashes)\n",
    "\n",
    "    print(f'It takes {num_flashes} flashes to stop.')\n",
    "    print('The estimated choice C* is \"%s\" with probability of %0.3f'\n",
    "        % (BOARD[max_loc], P_all[max_loc]))\n",
    "    print('For reference, the probability of the true target \"%s\" is %0.3f'\n",
    "        % (BOARD[target_loc], P_all[target_loc]))\n",
    "\n",
    "trail_perform # check results [passed!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_epochs_list = []\n",
    "# for i in range(6, 11):\n",
    "#     i = str(i) if i >= 10 else '0'+str(i)\n",
    "#     test_file = directory+f'/A{obj}/SE001/Test/RC/A{obj}_SE001RC_Test{i}.edf'\n",
    "#     raw_test_data = mne.io.read_raw_edf(test_file, preload=True, verbose=False)\n",
    "#     eeg_channels = mne.pick_channels_regexp(raw_test_data.info['ch_names'], 'EEG')\n",
    "#     raw_test_data.notch_filter(freqs=60, picks=eeg_channels, verbose=False)\n",
    "#     part_test_epochs = utils.get_core_epochs(raw_test_data)\n",
    "#     test_epochs_list.append(part_test_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ ] Plot AUC for each paticipates\n",
    "# [ ] Plot accuracy for each paticipates (DS x 1 and SS x 1)\n",
    "# [ ] Plot bits rate for each paticipates (DS)\n",
    "# [ ] Plot the number of flashes per each character for each participant (DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] Polich J. Updating P300: an integrative theory of P3a and P3b. Clin Neurophysiol. 2007;118(10):2128-2148. doi:10.1016/j.clinph.2007.04.019\n",
    "\n",
    "[2] Krusienski, Dean J., et al. “A Comparison of Classification Techniques for the P300 Speller.” Journal of Neural Engineering, Nov. 2006, pp. 299–305, https://doi.org/10.1088/1741-2560/3/4/007."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
