{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a CNN classifier for P300 speller\n",
    "\n",
    "<div style=\"text-align:justify; width: 97%\">\n",
    "Main reference: *CNN With Large Data Achieves True Zero-Training in Online P300 Brain-Computer Interface* by J. Lee et al. (2020)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "## Python standard libraries\n",
    "import math\n",
    "import random\n",
    "\n",
    "## Packages for computation and modelling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "from torcheeg.models import EEGNet\n",
    "import mne\n",
    "import pickle\n",
    "\n",
    "## Packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Self-defined packages\n",
    "from swlda import SWLDA\n",
    "from eegnet_utils import *\n",
    "\n",
    "# Magic command to reload packages whenever we run any later cells\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD = [[\"A\",    \"B\",  \"C\",   \"D\",    \"E\",    \"F\",     \"G\",    \"H\"    ],\n",
    "         [\"I\",    \"J\",  \"K\",   \"L\",    \"M\",    \"N\",     \"O\",    \"P\"    ],\n",
    "         [\"Q\",    \"R\",  \"S\",   \"T\",    \"U\",    \"V\",     \"W\",    \"X\"    ],\n",
    "         [\"Y\",    \"Z\",  \"Sp\",  \"1\",    \"2\",    \"3\",     \"4\",    \"5\"    ],\n",
    "         [\"6\",    \"7\",  \"8\",   \"9\",    \"0\",    \"Prd\",   \"Ret\",  \"Bs\"   ],\n",
    "         [\"?\",    \",\",  \";\",   \"\\\\\",   \"/\",    \"+\",     \"-\",    \"Alt\"  ],\n",
    "         [\"Ctrl\", \"=\",  \"Del\", \"Home\", \"UpAw\", \"End\",   \"PgUp\", \"Shft\" ],\n",
    "         [\"Save\", \"'\",  \"F2\",  \"LfAw\", \"DnAw\", \"RtAw\",  \"PgDn\", \"Pause\"],\n",
    "         [\"Caps\", \"F5\", \"Tab\", \"EC\",   \"Esc\",  \"email\", \"!\",    \"Sleep\"]]\n",
    "BOARD  = np.array(BOARD)\n",
    "N_ROWS = BOARD.shape[0]  # number of rows\n",
    "N_COLS = BOARD.shape[1]  # number of columns\n",
    "M = N_ROWS * N_COLS      # the number of chars on the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (833, 32)\n",
      "feature size is (952, 32)\n",
      "feature size is (833, 32)\n"
     ]
    }
   ],
   "source": [
    "paradigm        = 'RC' # display paradigm ('RC', 'CB', or 'RD')\n",
    "NUM_TIMESTAMPS  = 195  # number of timestamps in each window to record signals\n",
    "NUM_CHANNELS    = 32   # number of eletrode channels\n",
    "EPOCH_SIZE      = 1    # size of epoch, 1 means we don't aggregrate\n",
    "NUM_TRAIN_WORDS = 5    # number of training words for one participant\n",
    "NUM_TEST_WORDS  = 5    # number of testing words for one participant\n",
    "obj_indices     = ['01', '02', '03', '04', '05', '06', '07',\n",
    "                   '09', '14', '15', '16', '17', '19']\n",
    "\n",
    "train_X_list, train_Y_list, test_X_list, test_Y_list = [], [], [], []\n",
    "for obj in obj_indices:\n",
    "    directory = '/Users/zionshane/Desktop/Duke/Research/BCI_data/EDFData-StudyA'\n",
    "    obj_directory = directory + f'/A{obj}/SE001'\n",
    "\n",
    "    train_features,train_response = load_data(dir=obj_directory,\n",
    "                                              obj=obj,\n",
    "                                              num_timestamps=NUM_TIMESTAMPS,\n",
    "                                              epoch_size=EPOCH_SIZE,\n",
    "                                              num_channels=NUM_CHANNELS,\n",
    "                                              type=paradigm,\n",
    "                                              mode='train',\n",
    "                                              num_words=NUM_TRAIN_WORDS)\n",
    "    train_X_list.append(train_features)\n",
    "    train_Y_list.append(train_response.reshape((-1, 1)))\n",
    "\n",
    "    test_features,test_response   = load_data(dir=obj_directory,\n",
    "                                              obj=obj,\n",
    "                                              num_timestamps=NUM_TIMESTAMPS,\n",
    "                                              epoch_size=EPOCH_SIZE,\n",
    "                                              num_channels=NUM_CHANNELS,\n",
    "                                              type=paradigm,\n",
    "                                              mode='test',\n",
    "                                              num_words=NUM_TEST_WORDS)\n",
    "    test_X_list.append(test_features)\n",
    "    test_Y_list.append(test_response.reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.from_numpy(np.vstack(train_X_list))\n",
    "train_Y = torch.from_numpy(np.vstack(train_Y_list))\n",
    "test_X  = torch.from_numpy(np.vstack(test_X_list))\n",
    "test_Y  = torch.from_numpy(np.vstack(test_Y_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55692, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55692, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "EPOCHS = 500\n",
    "\n",
    "trainset, testset = TensorDataset(train_X, train_Y), TensorDataset(test_X, test_Y)\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader  = DataLoader(dataset=testset,  batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the EEGNet classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/zionshane/Desktop/Duke/Research/P300-BCI/P300-BCI/Task7/task7.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zionshane/Desktop/Duke/Research/P300-BCI/P300-BCI/Task7/task7.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m eegnet \u001b[39m=\u001b[39m EEGNet()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zionshane/Desktop/Duke/Research/P300-BCI/P300-BCI/Task7/task7.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m Model(eegnet, lr\u001b[39m=\u001b[39mLR)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zionshane/Desktop/Duke/Research/P300-BCI/P300-BCI/Task7/task7.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(trainloader\u001b[39m=\u001b[39;49mtrainloader, validloader\u001b[39m=\u001b[39;49mtestloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zionshane/Desktop/Duke/Research/P300-BCI/P300-BCI/Task7/task7.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                     epochs\u001b[39m=\u001b[39;49mEPOCHS, monitor\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39macc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mval_acc\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zionshane/Desktop/Duke/Research/P300-BCI/P300-BCI/Task7/task7.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plot_acc_and_loss(history\u001b[39m=\u001b[39mhistory)\n",
      "File \u001b[0;32m~/Desktop/Duke/Research/P300-BCI/P300-BCI/Task7/eegnet_utils.py:190\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, trainloader, validloader, epochs, monitor, only_print_finish_ep_num)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m trainloader:\n\u001b[1;32m    189\u001b[0m     x_batch, y_batch \u001b[39m=\u001b[39m x_batch\u001b[39m.\u001b[39mto(device, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat), y_batch\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 190\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x_batch)\n\u001b[1;32m    191\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses(pred, y_batch)\n\u001b[1;32m    192\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torcheeg/models/cnn/eegnet.py:121\u001b[0m, in \u001b[0;36mEEGNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    114\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m        x (torch.Tensor): EEG signal representation, the ideal input shape is :obj:`[n, 60, 151]`. Here, :obj:`n` corresponds to the batch size, :obj:`60` corresponds to :obj:`num_electrodes`, and :obj:`151` corresponds to :obj:`chunk_size`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39m        torch.Tensor[number of sample, number of classes]: the predicted probability that the samples belong to the classes.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock1(x)\n\u001b[1;32m    122\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock2(x)\n\u001b[1;32m    123\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 32]"
     ]
    }
   ],
   "source": [
    "eegnet = EEGNet().to(device)\n",
    "model = Model(eegnet, lr=LR)\n",
    "history = model.fit(trainloader=trainloader, validloader=testloader,\n",
    "                    epochs=EPOCHS, monitor=[\"acc\", \"val_acc\"])\n",
    "plot_acc_and_loss(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
