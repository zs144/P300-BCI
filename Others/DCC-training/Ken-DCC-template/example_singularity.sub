#!/bin/bash

## These Double # signs are interpreted by SLURM as comments
## while single # signs before SBATCH are not

# Job name:
#SBATCH --job-name=Singularity_Test
#
# Account: railabs or bashirlab
#SBATCH --account=railabs
#
# Partition: # It is default queue for now.
#Will be modified as nodes are added
#
#SBATCH --partition=defq 
#
# Number of nodes:
#SBATCH --nodes=1 
#
# Number of tasks (one for each GPU desired for use case) (example):
#SBATCH --ntasks=1
#
# Processors per task:
# Always at least 8X the number of GPUs
#SBATCH --cpus-per-task=8
#
# Number of GPUs:
#SBATCH --gpus=1
# Or (Use only one way either ^ or as in line below
##SBATCH --gres=gpu:1   
#
#SBATCH --no-kill # allow job to continue with dead task
#SBATCH --output=output-%j.out
#SBATCH --error=error-%j.err



module load singularity/singularity.module
export NVIDIA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES
## The $CUDA_VISIBLE_DEVICES will show which gpus are made available to your jobs
## the NVIDIA_VISIBLE_DEVICES is only for singularity 

echo "Testing singularity "
echo -n "hostname is "; hostname
echo "cpu/Task: $SLURM_CPUS_PER_TASK"
echo "visDev:   $CUDA_VISIBLE_DEVICES"
echo "cpu/gpu:  $SLURM_CPUS_PER_GPU"
echo "nvidia Visible Devices: $NVIDIA_VISIBLE_DEVICES"

## --nv flag will make GPUs (NVIDIA_VISIBLE_DEVICES) available to singularity container
## More at 
## https://docs.sylabs.io/guides/master/user-guide/gpu.html#nvidia-gpus-cuda-nvidia-container-cli

# Here you point to your singularity image file.
singularity exec --nv ./tensorflow_latest-gpu.sif python mnisttest.py

